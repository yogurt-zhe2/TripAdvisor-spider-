#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TripAdvisoræ™¯ç‚¹è¯„è®ºçˆ¬è™« - å¢å¼ºç‰ˆæœ¬
è§£å†³æ•°æ®åº“è¿æ¥ã€å¤šçº¿ç¨‹å†²çªã€å®Œæ•´é‡‡é›†ç­‰é—®é¢˜
"""

import requests
import json
import os
import time
import random
import urllib3
import csv
import re
import uuid
import argparse
import gc
import psutil
import pymysql
from datetime import datetime
from threading import Lock, RLock
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import threading

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================================ é…ç½®åŒº ================================
API_URL = "https://api.tripadvisor.cn/restapi/soa2/20997/getList"
OUTPUT_DIR = "attraction_comments"
PROGRESS_FILE = "progress.json"
COLLECTION_LOG_FILE = "collection_log.csv"

# MySQLæ•°æ®åº“é…ç½®ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
MYSQL_CONFIG = {
    'host': os.getenv('MYSQL_HOST', 'localhost'),
    'port': int(os.getenv('MYSQL_PORT', '3306')),
    'user': os.getenv('MYSQL_USER', ''),
    'password': os.getenv('MYSQL_PASSWORD', ''),
    'database': os.getenv('MYSQL_DATABASE', ''),
    'charset': os.getenv('MYSQL_CHARSET', 'utf8mb4'),
    'autocommit': (os.getenv('MYSQL_AUTOCOMMIT', 'true').lower() in ['1', 'true', 'yes']),
    'connect_timeout': int(os.getenv('MYSQL_CONNECT_TIMEOUT', '30')),
    'read_timeout': int(os.getenv('MYSQL_READ_TIMEOUT', '60')),
    'write_timeout': int(os.getenv('MYSQL_WRITE_TIMEOUT', '60'))
}

# è¯·æ±‚å¤´ï¼ˆéƒ¨åˆ†å­—æ®µå¯ç”±ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
HEADERS = {
    'accept': 'application/json, text/plain, */*',
    'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'content-type': 'application/json;charset=UTF-8',
    'origin': 'https://www.tripadvisor.cn',
    'referer': 'https://www.tripadvisor.cn/',
    'sec-ch-ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-site',
    'user-agent': os.getenv('TA_USER_AGENT', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
    'x-ta-uid': os.getenv('TA_X_TA_UID', ''),
    'cookie': os.getenv('TA_COOKIE', ''),
    'dnt': '1',
    'pragma': 'no-cache',
    'cache-control': 'no-cache'
}

# å…¨å±€å˜é‡
last_request_time = 0
processed_urls = set()
success_count = 0
failed_count = 0
SKIP_DB_OPERATION = False
SELECTED_LANGS = None
THREAD_COUNT = 15  # é»˜è®¤15çº¿ç¨‹

# é” - ä½¿ç”¨RLocké¿å…æ­»é”
file_lock = RLock()
progress_lock = RLock()
log_lock = RLock()
db_lock = RLock()  # æ•°æ®åº“æ“ä½œé”
request_lock = RLock()  # è¯·æ±‚é¢‘ç‡é”

# User-Agentåˆ—è¡¨ï¼ˆè‹¥è®¾ç½® TA_USER_AGENTï¼Œåˆ™ä¼˜å…ˆåŠ å…¥æ± é¦–ä½ï¼‰
USER_AGENTS = [
    os.getenv('TA_USER_AGENT', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
]

# ================================ ç½‘ç»œè¯·æ±‚æ¨¡å— ================================
def create_session():
    """åˆ›å»ºæ–°çš„session"""
    session = requests.Session()
    session.verify = False
    session.trust_env = False
    
    # ç®€å•çš„è¿æ¥æ± é…ç½®
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=1,
        pool_maxsize=1,
        max_retries=0
    )
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    
    return session

def make_request_with_retry(url, json_data, max_retries=5):
    """ç½‘ç»œè¯·æ±‚å‡½æ•° - å¢å¼ºç‰ˆï¼Œæ”¹è¿›é€€é¿ç­–ç•¥"""
    global last_request_time
    
    # é¢‘ç‡é™åˆ¶
    with request_lock:
        current_time = time.time()
        time_since_last = current_time - last_request_time
        min_interval = random.uniform(1.0, 2.0)
        if time_since_last < min_interval:
            sleep_time = min_interval - time_since_last
            time.sleep(sleep_time)
        last_request_time = time.time()
    
    # æ¯æ¬¡åˆ›å»ºæ–°çš„session
    session = create_session()
    
    # è½®æ¢User-Agent
    headers = dict(HEADERS)
    headers['user-agent'] = random.choice(USER_AGENTS)
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ“¡ å‘é€è¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries})...")
            
            # å‘é€è¯·æ±‚
            resp = session.post(
                url, 
                headers=headers, 
                json=json_data, 
                timeout=(15, 45)  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            if resp.status_code == 200:
                print(f"âœ… è¯·æ±‚æˆåŠŸ")
                try:
                    data = resp.json()
                    return resp
                except:
                    print("âš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆJSON")
                    if attempt < max_retries - 1:
                        # æŒ‡æ•°é€€é¿ç­–ç•¥
                        wait_time = min(30, 2 ** attempt + random.uniform(0, 1))
                        print(f"â³ ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return None
            elif resp.status_code in (429, 403):
                # é¢‘ç‡é™åˆ¶ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                wait_time = min(60, 5 * (2 ** attempt) + random.uniform(0, 5))
                print(f"âš ï¸  é¢‘ç‡é™åˆ¶ ({resp.status_code})ï¼Œç­‰å¾… {wait_time:.1f}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
                headers['user-agent'] = random.choice(USER_AGENTS)
                continue
            elif resp.status_code >= 500:
                # æœåŠ¡å™¨é”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                wait_time = min(30, 3 * (2 ** attempt) + random.uniform(0, 3))
                print(f"âš ï¸  æœåŠ¡å™¨é”™è¯¯ ({resp.status_code})ï¼Œç­‰å¾… {wait_time:.1f}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
                continue
            else:
                print(f"âš ï¸ çŠ¶æ€ç : {resp.status_code}")
                if attempt < max_retries - 1:
                    wait_time = min(20, 2 ** attempt + random.uniform(0, 2))
                    print(f"â³ ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    return None
                    
        except requests.exceptions.Timeout as e:
            print(f"âš ï¸ è¶…æ—¶: {type(e).__name__}")
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿ç­–ç•¥
                wait_time = min(30, 3 * (2 ** attempt) + random.uniform(0, 3))
                print(f"â³ ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue
            else:
                return None
                
        except requests.exceptions.ConnectionError as e:
            print(f"âš ï¸ è¿æ¥é”™è¯¯: {e}")
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿ç­–ç•¥
                wait_time = min(60, 5 * (2 ** attempt) + random.uniform(0, 5))
                print(f"â³ ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue
            else:
                return None
                
        except Exception as e:
            print(f"âš ï¸ æœªçŸ¥é”™è¯¯: {type(e).__name__} - {e}")
            if attempt < max_retries - 1:
                wait_time = min(15, 2 ** attempt + random.uniform(0, 2))
                print(f"â³ ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue
            else:
                return None
    
    return None

# ================================ æ•°æ®è·å–æ¨¡å— ================================
def get_available_langs(location_id, url=None):
    """è·å–è¯¥æ™¯ç‚¹å¯ç”¨çš„è¯­è¨€åˆ—è¡¨"""
    if url:
        print(f"\nğŸ” æ­£åœ¨è·å–è¯­è¨€åˆ—è¡¨... | URL: {url}")
    else:
        print(f"\nğŸ” æ­£åœ¨è·å–è¯­è¨€åˆ—è¡¨... | æ™¯ç‚¹ID: {location_id}")
    
    payload = {
        "frontPage": "USER_REVIEWS",
        "locationId": int(location_id),
        "selected": {"langs": []},
        "pageInfo": {"num": 1, "size": 1}
    }
    langs = []
    location_info = None
    try:
        resp = make_request_with_retry(API_URL, payload)
        if not resp:
            return langs, location_info
        data = resp.json()
        
        # è¯­è¨€èšåˆ - æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        print(f"ğŸ“Š è¯­è¨€èšåˆä¿¡æ¯:")
        for agg in data.get('langAggs', []) or []:
            key = agg.get('key')
            count = agg.get('count', 0)
            print(f"  - {key}: {count}æ¡è¯„è®º")
            if key and key != 'all' and count > 0:
                langs.append(key)
        
        # ä»ä»»æ„detailsä¸­è·å–locationInfo
        details = data.get('details', []) or []
        if details:
            loc = details[0].get('locationInfo', {})
            if loc:
                location_info = {
                    "attractionName": loc.get('name', 'æœªçŸ¥æ™¯ç‚¹'),
                    "cityName": loc.get('cityName', 'æœªçŸ¥åŸå¸‚'),
                    "cityId": loc.get('cityId', 0),
                    "address": loc.get('address', 'æœªçŸ¥åœ°å€'),
                    "rating": str(loc.get('rating', 'N/A')),
                    "reviewCount": str(loc.get('reviewCount', '0'))
                }
                if url:
                    print(f"ğŸ›ï¸ æ™¯ç‚¹ä¿¡æ¯: {location_info['attractionName']}({location_info['cityName']}) | è¯„åˆ†:{location_info['rating']} | æ€»è¯„è®º:{location_info['reviewCount']} | URL: {url}")
                else:
                    print(f"ğŸ›ï¸ æ™¯ç‚¹ä¿¡æ¯: {location_info['attractionName']}({location_info['cityName']}) | è¯„åˆ†:{location_info['rating']} | æ€»è¯„è®º:{location_info['reviewCount']}")
    except Exception as e:
        print(f"âš ï¸  è·å–è¯­è¨€åˆ—è¡¨å¼‚å¸¸: {e}")
    return langs, location_info

def get_reviews_and_info(location_id, langs=None, max_pages_per_lang=10, url=None):
    """è·å–æŒ‡å®šåœ°ç‚¹çš„è¯„è®ºå’Œæ™¯ç‚¹ä¿¡æ¯ - å®Œæ•´é‡‡é›†ç‰ˆ"""
    comments = []
    location_info = None

    # è§£æè¯­è¨€åˆ—è¡¨
    languages_to_fetch = []
    if not langs or (isinstance(langs, list) and len(langs) == 1 and langs[0] == 'all'):
        # ä»"å…¨éƒ¨è¯„è®º"é¡µé¢è·å–è¯­è¨€åˆ—è¡¨å’Œæ™¯ç‚¹ä¿¡æ¯
        languages_to_fetch, location_info = get_available_langs(location_id, url)
        if not languages_to_fetch:
            languages_to_fetch = ['all']
    else:
        languages_to_fetch = langs

    if url:
        print(f"ğŸ” è·å–åˆ°è¯­è¨€åˆ—è¡¨: {languages_to_fetch} | URL: {url}")
    else:
        print(f"ğŸ” è·å–åˆ°è¯­è¨€åˆ—è¡¨: {languages_to_fetch} | æ™¯ç‚¹ID: {location_id}")

    # é‡è¦ï¼šå§‹ç»ˆä»"å…¨éƒ¨è¯„è®º"å¼€å§‹é‡‡é›†ï¼Œç¡®ä¿è·å–æ‰€æœ‰è¯„è®º
    print(f"\nğŸŒ å¼€å§‹é‡‡é›†å…¨éƒ¨è¯„è®º (all) | URL: {url if url else f'æ™¯ç‚¹ID: {location_id}'}")
    
    page_num = 1
    total_comments = 0
    empty_pages_count = 0
    consecutive_empty_pages = 0  # è¿ç»­ç©ºé¡µè®¡æ•°
    
    # ä»"å…¨éƒ¨è¯„è®º"é¡µé¢æ— é™åˆ¶é‡‡é›†
    while True:
        # ä½¿ç”¨"all"æ¨¡å¼ï¼Œä¸æŒ‡å®šç‰¹å®šè¯­è¨€
        payload = {
            "frontPage": "USER_REVIEWS",
            "locationId": int(location_id),
            "selected": {
                "airlineIds": [],
                "airlineSeatIds": [],
                "langs": [],  # ç©ºåˆ—è¡¨è¡¨ç¤ºå…¨éƒ¨è¯­è¨€
                "ratings": [],
                "seasons": [],
                "tripTypes": [],
                "airlineLevel": []
            },
            "pageInfo": {"num": page_num, "size": 10}
        }

        try:
            response = make_request_with_retry(API_URL, payload)
            if not response:
                if url:
                    print(f"âŒ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥ | URL: {url}")
                else:
                    print(f"âŒ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥")
                break
                
            data = response.json()
            reviews_data = data.get('details', [])
            
            if not reviews_data:
                empty_pages_count += 1
                consecutive_empty_pages += 1
                if url:
                    print(f"ğŸ“„ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µæ— æ•°æ® (è¿ç»­{consecutive_empty_pages}é¡µ) | URL: {url}")
                else:
                    print(f"ğŸ“„ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µæ— æ•°æ® (è¿ç»­{consecutive_empty_pages}é¡µ)")
                
                # è¿ç»­5é¡µæ— æ•°æ®æˆ–æ€»ç©ºé¡µè¶…è¿‡10é¡µåˆ™åœæ­¢
                if consecutive_empty_pages >= 5 or empty_pages_count >= 10:
                    if url:
                        print(f"ğŸ›‘ å…¨éƒ¨è¯„è®ºè¿ç»­{consecutive_empty_pages}é¡µæ— æ•°æ®ï¼Œåœæ­¢é‡‡é›† | URL: {url}")
                    else:
                        print(f"ğŸ›‘ å…¨éƒ¨è¯„è®ºè¿ç»­{consecutive_empty_pages}é¡µæ— æ•°æ®ï¼Œåœæ­¢é‡‡é›†")
                    break
                
                page_num += 1
                time.sleep(random.uniform(1, 2))
                continue

            # åˆå§‹åŒ–location_info
            if not location_info:
                first = reviews_data[0]
                loc_info = first.get('locationInfo') if isinstance(first, dict) else None
                if loc_info:
                    location_info = {
                        "attractionName": loc_info.get('name', 'æœªçŸ¥æ™¯ç‚¹'),
                        "cityName": loc_info.get('cityName', 'æœªçŸ¥åŸå¸‚'),
                        "cityId": loc_info.get('cityId', 0),
                        "address": loc_info.get('address', 'æœªçŸ¥åœ°å€'),
                        "rating": str(loc_info.get('rating', 'N/A')),
                        "reviewCount": str(loc_info.get('reviewCount', '0'))
                    }

            # è§£æè¯„è®º
            page_comments = []
            for review in reviews_data:
                try:
                    member_info = review.get('memberInfo', {}) if isinstance(review, dict) else {}
                    page_comments.append({
                        "userReviewId": str(review.get("userReviewId", "")),
                        "username": member_info.get("displayName") or member_info.get("username", "Tripadvisorç”¨æˆ·"),
                        "userRating": review.get("rating", 0),
                        "title": review.get("title", ""),
                        "tripTypeString": review.get("tripTypeString", ""),
                        "content": review.get("content", ""),
                        "lang": review.get("lang", ""),
                        "submitTime": review.get("submitTime", ""),
                        "attribution": review.get("attribution", "")
                    })
                except Exception as e:
                    print(f"âš ï¸  è¯„è®ºè§£æé”™è¯¯: {e}")
                    continue

            comments.extend(page_comments)
            total_comments += len(page_comments)
            consecutive_empty_pages = 0  # é‡ç½®è¿ç»­ç©ºé¡µè®¡æ•°
            if url:
                print(f"ğŸ“„ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µ: {len(page_comments)}æ¡è¯„è®º | URL: {url}")
            else:
                print(f"ğŸ“„ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µ: {len(page_comments)}æ¡è¯„è®º")
            
            page_num += 1
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            if url:
                print(f"âŒ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µå¤„ç†å¤±è´¥: {e} | URL: {url}")
            else:
                print(f"âŒ å…¨éƒ¨è¯„è®ºç¬¬ {page_num} é¡µå¤„ç†å¤±è´¥: {e}")
            break
    
    # æ˜¾ç¤ºé‡‡é›†ç»“æœ
    if total_comments > 0:
        if url:
            print(f"  âœ… å…¨éƒ¨è¯„è®ºé‡‡é›†å®Œæˆ: {total_comments}æ¡è¯„è®º | URL: {url}")
        else:
            print(f"  âœ… å…¨éƒ¨è¯„è®ºé‡‡é›†å®Œæˆ: {total_comments}æ¡è¯„è®º | æ™¯ç‚¹ID: {location_id}")

    return comments, location_info

# ================================ æ•°æ®åº“æ¨¡å— - å¢å¼ºç‰ˆ ================================
def get_db_connection_with_retry(max_retries=5):
    """è·å–æ•°æ®åº“è¿æ¥ - å¸¦é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”— è¿æ¥æ•°æ®åº“ (å°è¯• {attempt + 1}/{max_retries})...")
            connection = pymysql.connect(**MYSQL_CONFIG)
            connection.autocommit(True)
            
            # æµ‹è¯•è¿æ¥
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                result = cursor.fetchone()
                if result and result[0] == 1:
                    print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
                    return connection
                else:
                    raise Exception("è¿æ¥æµ‹è¯•å¤±è´¥")
                    
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿ç­–ç•¥
                wait_time = min(60, 5 * (2 ** attempt) + random.uniform(0, 5))
                print(f"â³ {wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print("âŒ æ•°æ®åº“è¿æ¥æœ€ç»ˆå¤±è´¥")
                return None
    
    return None

def test_db_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    connection = get_db_connection_with_retry()
    if connection:
        try:
            connection.close()
            return True
        except:
            pass
    return False

def execute_db_operation_with_retry(operation_func, *args, **kwargs):
    """æ‰§è¡Œæ•°æ®åº“æ“ä½œçš„é€šç”¨å‡½æ•°ï¼Œæ¯æ¬¡é‡æ–°è¿æ¥ï¼Œå¸¦é‡è¯•"""
    max_retries = 5
    for attempt in range(max_retries):
        connection = None
        try:
            connection = get_db_connection_with_retry()
            if not connection:
                if attempt < max_retries - 1:
                    wait_time = min(30, 3 * (2 ** attempt))
                    print(f"ğŸ”„ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    return False
            
            result = operation_func(connection, *args, **kwargs)
            return result
            
        except (pymysql.Error, Exception) as e:
            print(f"âš ï¸  æ•°æ®åº“æ“ä½œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            
            if attempt < max_retries - 1:
                wait_time = min(30, 3 * (2 ** attempt))
                print(f"â³ {wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print("âŒ æ•°æ®åº“æ“ä½œæœ€ç»ˆå¤±è´¥")
                return False
        finally:
            # ç¡®ä¿è¿æ¥å…³é—­
            if connection:
                try:
                    connection.close()
                except:
                    pass
    
    return False

def _insert_record_operation(connection, url, comment_count, json_filename, location_info):
    """å®é™…çš„æ’å…¥è®°å½•æ“ä½œ"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    relative_path = f"./{OUTPUT_DIR}/{json_filename}"
    
    # å‡†å¤‡æ’å…¥æ•°æ® - ç¡®ä¿å­—æ®µå®Œæ•´
    insert_data = {
        "é‡‡é›†ç½‘ç«™": "TripAdvisor",
        "url": url,
        "é‡‡é›†äºº": os.getenv('COLLECTOR_NAME', ''),
        "é‡‡é›†æ—¶é—´": current_time,
        "è¯„è®ºæ•°": str(comment_count),
        "å­˜å‚¨åœ°å€": relative_path
    }
    
    # æ„å»ºSQLæ’å…¥è¯­å¥
    columns = list(insert_data.keys())
    values = list(insert_data.values())
    placeholders = ', '.join(['%s'] * len(columns))
    column_names = ', '.join([f'`{col}`' for col in columns])
    
    table_name = os.getenv('MYSQL_TABLE', 'collection_table')
    sql = f"INSERT INTO {table_name} ({column_names}) VALUES ({placeholders})"
    
    with connection.cursor() as cursor:
        cursor.execute(sql, values)
    
    print(f"ğŸ’¾ æ•°æ®åº“è®°å½•å·²æ’å…¥: {location_info.get('attractionName', 'æœªçŸ¥æ™¯ç‚¹')}")
    return True

def insert_collection_record_to_db(url, comment_count, json_filename, location_info):
    """å°†é‡‡é›†è®°å½•æ’å…¥æ•°æ®åº“ - å¸¦é‡è¯•"""
    return execute_db_operation_with_retry(_insert_record_operation, url, comment_count, json_filename, location_info)

def _delete_record_operation(connection, url, location_info):
    """å®é™…çš„åˆ é™¤è®°å½•æ“ä½œ"""
    table_name = os.getenv('MYSQL_TABLE', 'collection_table')
    sql = f"DELETE FROM {table_name} WHERE url = %s"
    
    with connection.cursor() as cursor:
        cursor.execute(sql, (url,))
    
    print(f"ğŸ”„ æ•°æ®åº“è®°å½•å·²å›æ»š: {location_info.get('attractionName', 'æœªçŸ¥æ™¯ç‚¹')}")
    return True

def delete_collection_record_from_db(url, location_info):
    """ä»æ•°æ®åº“åˆ é™¤é‡‡é›†è®°å½•ï¼ˆç”¨äºå›æ»šï¼‰- å¸¦é‡è¯•"""
    return execute_db_operation_with_retry(_delete_record_operation, url, location_info)

# ================================ è¿›åº¦ç®¡ç†æ¨¡å— ================================
def save_progress():
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    progress_data = {
        "processed_urls": list(processed_urls),
        "success_count": success_count,
        "failed_count": failed_count,
        "timestamp": time.time()
    }
    
    try:
        with file_lock:
            with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

def load_progress():
    """åŠ è½½å¤„ç†è¿›åº¦"""
    global processed_urls, success_count, failed_count
    
    if not os.path.exists(PROGRESS_FILE):
        return
    
    try:
        with file_lock:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
                
        processed_urls = set(progress_data.get("processed_urls", []))
        success_count = progress_data.get("success_count", 0)
        failed_count = progress_data.get("failed_count", 0)
        
        print(f"ğŸ“ å·²åŠ è½½è¿›åº¦: å·²å¤„ç† {len(processed_urls)} ä¸ªURL, æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
    except Exception as e:
        print(f"âš ï¸  åŠ è½½è¿›åº¦å¤±è´¥: {e}")

# ================================ é‡‡é›†è®°å½•æ¨¡å— ================================
def init_collection_log():
    """åˆå§‹åŒ–é‡‡é›†è®°å½•CSVæ–‡ä»¶"""
    if not os.path.exists(COLLECTION_LOG_FILE):
        with log_lock:
            with open(COLLECTION_LOG_FILE, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow([
                    "é‡‡é›†ç½‘ç«™", "url", "é‡‡é›†äºº", "é‡‡é›†æ—¶é—´", "è¯„è®ºæ•°", "å­˜å‚¨åœ°å€"
                ])
        print(f"ğŸ“ å·²åˆ›å»ºé‡‡é›†è®°å½•æ–‡ä»¶: {COLLECTION_LOG_FILE}")

def log_collection_record(url, comment_count, json_filename, location_info):
    """è®°å½•é‡‡é›†ä¿¡æ¯åˆ°CSV - çº¿ç¨‹å®‰å…¨"""
    with log_lock:
        try:
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            relative_path = f"./{OUTPUT_DIR}/{json_filename}"
            
            record = [
                "TripAdvisor",
                url,
                os.getenv('COLLECTOR_NAME', ''),
                current_time,
                str(comment_count),
                relative_path
            ]
            
            with open(COLLECTION_LOG_FILE, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(record)
                
        except Exception as e:
            print(f"âš ï¸  è®°å½•é‡‡é›†ä¿¡æ¯å¤±è´¥: {e}")

# ================================ ç³»ç»Ÿç›‘æ§æ¨¡å— ================================
def get_memory_usage():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        return memory_info.rss / 1024 / 1024  # MB
    except:
        return 0

def log_memory_usage():
    """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory_mb = get_memory_usage()
    if memory_mb > 500:  # è¶…è¿‡500MBæ—¶è­¦å‘Š
        print(f"âš ï¸  å†…å­˜ä½¿ç”¨: {memory_mb:.1f}MB")

# ================================ æ ¸å¿ƒå¤„ç†æ¨¡å— ================================
def extract_ids_from_url(url):
    """ä»URLä¸­æå–åŸå¸‚IDå’Œåœ°ç‚¹ID"""
    match = re.search(r'-g(\d+)-d(\d+)-', url)
    if match:
        return match.groups()  # (city_id, location_id)
    return None, None

def generate_safe_filename(attraction_name, location_id):
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å - ä¸ä¸»ç¨‹åºä¿æŒä¸€è‡´"""
    # ç§»é™¤éæ³•å­—ç¬¦
    safe_name = re.sub(r'[\\/*?:"<>|\x00-\x1f\x7f-\x9f]', "", attraction_name)
    safe_name = safe_name.strip()[:100]  # ä¸ä¸»ç¨‹åºä¿æŒä¸€è‡´çš„é•¿åº¦é™åˆ¶
    
    if not safe_name:
        safe_name = f"attraction_{location_id}"
    
    # ä¸ä¸»ç¨‹åºä¿æŒä¸€è‡´çš„å‘½åæ ¼å¼ï¼šæ™¯ç‚¹å_UUIDå‰8ä½.json
    random_uuid = uuid.uuid4()
    filename = f"{safe_name}_{random_uuid.hex[:8]}.json"
    
    return filename

def process_single_attraction(url):
    """å¤„ç†å•ä¸ªæ™¯ç‚¹çš„æ•°æ®é‡‡é›† - çº¿ç¨‹å®‰å…¨ç‰ˆ"""
    global success_count, failed_count
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        with progress_lock:
            if url in processed_urls:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†çš„URL: {url}")
                return True
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ å¼€å§‹å¤„ç†æ™¯ç‚¹: {url}")
        print(f"{'='*80}")
        
        # æå–ID
        city_id, location_id = extract_ids_from_url(url)
        if not location_id:
            print(f"âŒ æ— æ³•ä»URLæå–ID: {url}")
            with progress_lock:
                processed_urls.add(url)
                failed_count += 1
            return False
        
        print(f"ğŸ“ æå–åˆ°ID: {location_id} | URL: {url}")
        
        # è·å–è¯„è®ºå’Œæ™¯ç‚¹ä¿¡æ¯
        print(f"ğŸ” æ­£åœ¨è·å–æ™¯ç‚¹ä¿¡æ¯... | URL: {url}")
        comments, location_info = get_reviews_and_info(location_id, langs=SELECTED_LANGS, url=url)
        
        # å¦‚æœæ²¡æœ‰è·å–åˆ°æ™¯ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not location_info:
            location_info = {
                "attractionName": f"æ™¯ç‚¹_{location_id}",
                "cityName": "æœªçŸ¥åŸå¸‚",
                "cityId": int(city_id) if city_id else 0,
                "address": "æœªçŸ¥åœ°å€",
                "rating": "N/A",
                "reviewCount": str(len(comments))
            }
            print(f"âš ï¸  ä½¿ç”¨é»˜è®¤æ™¯ç‚¹ä¿¡æ¯ | URL: {url}")
        
        print(f"ğŸ›ï¸ æ™¯ç‚¹ä¿¡æ¯: {location_info['attractionName']}({location_info['cityName']}) | æ€»è¯„è®º:{len(comments)}æ¡ | URL: {url}")
        
        # æ•´åˆæ•°æ®
        final_data = {
            "url": url,
            "cityName": location_info['cityName'],
            "cityId": int(city_id) if city_id else location_info['cityId'],
            "attractionName": location_info['attractionName'],
            "address": location_info['address'],
            "reviewCount": location_info['reviewCount'],
            "rating": location_info['rating'],
            "comments": comments,
            "é‡‡é›†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "é‡‡é›†äºº": os.getenv('COLLECTOR_NAME', '')
        }
        
        # ä¿å­˜æ–‡ä»¶ - ä½¿ç”¨æ™¯ç‚¹åç§°+UUIDå‘½åï¼Œä¸ä¸»ç¨‹åºä¿æŒä¸€è‡´
        attraction_name = location_info['attractionName']
        # æ›´ä¸¥æ ¼çš„æ–‡ä»¶åæ¸…ç†ï¼Œç¡®ä¿å®‰å…¨
        safe_filename = re.sub(r'[\\/*?:"<>|\x00-\x1f\x7f-\x9f]', "", attraction_name)
        safe_filename = safe_filename.strip()[:100]  # é™åˆ¶é•¿åº¦
        if not safe_filename:
            safe_filename = f"attraction_{location_id}"
        random_uuid = uuid.uuid4()
        filename = f"{safe_filename}_{random_uuid.hex[:8]}.json"
        filepath = os.path.join(OUTPUT_DIR, filename)
        
        # å…ˆå°è¯•æ’å…¥æ•°æ®åº“ï¼ˆæ•°æ®åº“æ˜¯æƒå¨ï¼‰
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®... | æ™¯ç‚¹: {location_info['attractionName']} | URL: {url}")
        if SKIP_DB_OPERATION:
            db_success = True
            print(f"âš ï¸  è·³è¿‡æ•°æ®åº“æ“ä½œ | URL: {url}")
        else:
            db_success = insert_collection_record_to_db(url, len(comments), filename, location_info)
        
        if db_success:
            # æ•°æ®åº“æ’å…¥æˆåŠŸï¼Œå†ä¿å­˜JSONæ–‡ä»¶
            save_success = False
            for save_attempt in range(3):
                try:
                    with file_lock:
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(final_data, f, ensure_ascii=False, indent=4)
                    print(f"ğŸ’¾ JSONæ–‡ä»¶ä¿å­˜æˆåŠŸ: {filename} | æ™¯ç‚¹: {location_info['attractionName']} | URL: {url}")
                    save_success = True
                    break
                except Exception as e:
                    if save_attempt < 2:
                        print(f"âš ï¸  JSONä¿å­˜å¤±è´¥ï¼Œé‡è¯• ({save_attempt + 1}/3): {e} | URL: {url}")
                        time.sleep(1)
                    else:
                        print(f"âŒ JSONä¿å­˜æœ€ç»ˆå¤±è´¥: {e} | URL: {url}")
            
            if save_success:
                # JSONä¿å­˜æˆåŠŸï¼Œå†å†™å…¥CSV
                log_collection_record(url, len(comments), filename, location_info)
                with progress_lock:
                    success_count += 1
                print(f"âœ… å®Œæ•´ä¿å­˜æˆåŠŸ: æ•°æ®åº“ + JSON + CSV | æ™¯ç‚¹: {location_info['attractionName']} | URL: {url}")
            else:
                # JSONä¿å­˜å¤±è´¥ï¼Œéœ€è¦å›æ»šæ•°æ®åº“
                if not SKIP_DB_OPERATION:
                    print(f"âŒ JSONä¿å­˜å¤±è´¥ï¼Œæ­£åœ¨å›æ»šæ•°æ®åº“è®°å½•... | URL: {url}")
                    rollback_success = delete_collection_record_from_db(url, location_info)
                    if rollback_success:
                        print(f"âœ… æ•°æ®åº“å›æ»šæˆåŠŸ | URL: {url}")
                    else:
                        print(f"âš ï¸  æ•°æ®åº“å›æ»šå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç† | URL: {url}")
                with progress_lock:
                    failed_count += 1
        else:
            # æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œä¸ä¿å­˜JSONå’ŒCSV
            print(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥ï¼Œè·³è¿‡JSONå’ŒCSVä¿å­˜ | URL: {url}")
            with progress_lock:
                failed_count += 1
        
        # æ¸…ç†å†…å­˜
        del final_data
        del comments
        gc.collect()
        
        # æ›´æ–°è¿›åº¦
        with progress_lock:
            processed_urls.add(url)
        
        # å¤„ç†å®Œæˆåé—´éš”
        # è¦†ç›–ç‡åˆ†æ
        if location_info and location_info['reviewCount'] != '0':
            total_reviews = int(location_info['reviewCount'])
            collected_reviews = len(comments)
            coverage = (collected_reviews / total_reviews) * 100
            print(f"\nğŸ“Š é‡‡é›†è¦†ç›–ç‡åˆ†æ | URL: {url}")
            print(f"  ğŸ“ ç½‘ç«™æ˜¾ç¤ºæ€»è¯„è®ºæ•°: {total_reviews}")
            print(f"  ğŸ“„ å®é™…é‡‡é›†è¯„è®ºæ•°: {collected_reviews}")
            print(f"  ğŸ“ˆ é‡‡é›†è¦†ç›–ç‡: {coverage:.1f}%")
            
            if coverage < 90:
                print(f"  âš ï¸ è¦†ç›–ç‡è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨é—æ¼ | URL: {url}")
            else:
                print(f"  âœ… è¦†ç›–ç‡è‰¯å¥½ | URL: {url}")
        
        print(f"ğŸ‰ æ™¯ç‚¹å¤„ç†å®Œæˆ: {location_info['attractionName']} | URL: {url}")
        time.sleep(random.uniform(2, 4))
        return True
                
    except Exception as e:
        print(f"âŒ å¤„ç†æ™¯ç‚¹å¤±è´¥ ({url}): {e}")
        # å³ä½¿å¤±è´¥ä¹Ÿè¦è®°å½•è¿›åº¦ï¼Œé¿å…é‡å¤å¤„ç†
        with progress_lock:
            processed_urls.add(url)
            failed_count += 1
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        # çŸ­æš‚ç­‰å¾…åç»§ç»­
        time.sleep(random.uniform(1, 3))
        return False

# ================================ ä¸»ç¨‹åº ================================
def read_urls_from_csv(csv_path):
    """ä»CSVæ–‡ä»¶è¯»å–URLåˆ—è¡¨"""
    urls = []
    if not os.path.exists(csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return urls
        
    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row.get('url'):
                urls.append(row['url'].strip())
    
    print(f"ğŸ“– ä»CSVè¯»å–åˆ° {len(urls)} æ¡URL")
    return urls

def create_sample_csv():
    """åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶"""
    sample_urls = [
        "https://www.tripadvisor.cn/Attraction_Review-g297701-d3683097-Reviews-Bali_Private_Tour_Id-Ubud_Gianyar_Regency_Bali.html",
        "https://www.tripadvisor.cn/Attraction_Review-g15880600-d20087279-Reviews-Heavenly_Spa_By_Westin_Ubud-Singakerta_Ubud_Gianyar_Regency_Bali.html",
        "https://www.tripadvisor.cn/Attraction_Review-g3961414-d16858110-Reviews-Bali_Experience_Adventure_Tour-Singapadu_Sukawati_Gianyar_Regency_Bali.html"
    ]
    
    with open('attraction_urls.csv', 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(["url"])
        for url in sample_urls:
            writer.writerow([url])
    
    print(f"ğŸ“ å·²åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶: attraction_urls.csv ({len(sample_urls)}æ¡URL)")

def main():
    """ä¸»ç¨‹åºå…¥å£ - å¢å¼ºç‰ˆ"""
    global success_count, failed_count, processed_urls, SELECTED_LANGS, SKIP_DB_OPERATION, THREAD_COUNT
    
    # åˆè§„ä¸ä½¿ç”¨é™åˆ¶æç¤ºæ¨ªå¹…
    print("\n" + "="*80)
    print("âš–ï¸  åˆè§„ä¸ä½¿ç”¨é™åˆ¶æé†’")
    print("- æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ä¸ç§‘ç ”ç”¨é€”ï¼Œä¸¥ç¦ä»»ä½•å½¢å¼çš„å•†ä¸šä½¿ç”¨ã€‚")
    print("- è¯·éµå®ˆ Tripadvisor æœåŠ¡æ¡æ¬¾ã€robots.txt ä¸è®¿é—®é¢‘ç‡é™åˆ¶ï¼Œä¸å¾—ç»•è¿‡åçˆ¬ã€‚")
    print("- ä½¿ç”¨æœ¬ä»£ç éœ€éµå®ˆæ‰€åœ¨åœ°ä¸æ•°æ®æ¥æºåœ°æ³•å¾‹æ³•è§„ï¼Œé£é™©è‡ªæ‹…ã€‚")
    print("è¯¦è§ README.md ä¸ DISCLAIMER.mdã€‚")
    print("="*80 + "\n")

    parser = argparse.ArgumentParser(description='TripAdvisoræ™¯ç‚¹è¯„è®ºçˆ¬è™« - å¢å¼ºç‰ˆæœ¬')
    parser.add_argument('--csv', default='attraction_urls.csv', help='URL CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--threads', type=int, default=3, help='å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤3')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰3ä¸ªURL')
    parser.add_argument('--create-sample', action='store_true', help='åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶')
    parser.add_argument('--reset-progress', action='store_true', help='é‡ç½®è¿›åº¦ï¼Œä»å¤´å¼€å§‹')
    parser.add_argument('--show-progress', action='store_true', help='æ˜¾ç¤ºå½“å‰è¿›åº¦å¹¶é€€å‡º')
    parser.add_argument('--langs', default='all', help='è¯­è¨€åˆ—è¡¨ï¼Œä¾‹å¦‚ zhCN,en,frï¼›é»˜è®¤ all è¡¨ç¤ºå…¨éƒ¨è¯­è¨€')
    parser.add_argument('--limit', type=int, default=None, help='ä»…å¤„ç†å‰Nä¸ªURLï¼Œç”¨äºæµ‹è¯•')
    parser.add_argument('--no-db', action='store_true', help='è·³è¿‡æ•°æ®åº“æ“ä½œï¼Œä»…ä¿å­˜JSONå’ŒCSV')
    args = parser.parse_args()

    # è®¾ç½®çº¿ç¨‹æ•°
    THREAD_COUNT = args.threads

    # è¯­è¨€è®¾ç½®
    if args.langs.strip().lower() == 'all':
        SELECTED_LANGS = ['all']
        print('ğŸŒ è¯­è¨€è®¾ç½®: å…¨éƒ¨è¯­è¨€')
    else:
        SELECTED_LANGS = [x.strip() for x in args.langs.split(',') if x.strip()]
        print(f"ğŸŒ è¯­è¨€è®¾ç½®: {SELECTED_LANGS}")

    # æ˜¾ç¤ºè¿›åº¦
    if args.show_progress:
        if os.path.exists(PROGRESS_FILE):
            load_progress()
            print(f"ğŸ“Š å½“å‰è¿›åº¦:")
            print(f"   - å·²å¤„ç†: {len(processed_urls)} ä¸ªURL")
            print(f"   - æˆåŠŸ: {success_count}")
            print(f"   - å¤±è´¥: {failed_count}")
        else:
            print("ğŸ“Š å°šæœªå¼€å§‹å¤„ç†")
        return

    # é‡ç½®è¿›åº¦
    if args.reset_progress:
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
            print("ğŸ”„ è¿›åº¦å·²é‡ç½®")
        return

    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    if args.create_sample:
        create_sample_csv()
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # æ•°æ®åº“è®¾ç½®
    if args.no_db:
        print("âš ï¸  è·³è¿‡æ•°æ®åº“æ“ä½œæ¨¡å¼")
        SKIP_DB_OPERATION = True
    else:
        print("ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥...")
        if not test_db_connection():
            print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        SKIP_DB_OPERATION = False
    
    # åˆå§‹åŒ–é‡‡é›†è®°å½•æ–‡ä»¶
    init_collection_log()

    # åŠ è½½è¿›åº¦
    load_progress()

    # è¯»å–URL
    all_urls = read_urls_from_csv(args.csv)
    if not all_urls:
        print("âŒ CSVä¸­æ²¡æœ‰URLï¼Œå¯ä»¥ä½¿ç”¨ --create-sample åˆ›å»ºç¤ºä¾‹æ–‡ä»¶")
        return

    # è¿‡æ»¤å·²å¤„ç†çš„URL
    pending_urls = [url for url in all_urls if url not in processed_urls]
 
    if not pending_urls:
        print("âœ… æ‰€æœ‰URLéƒ½å·²å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        return

    # æµ‹è¯•æ¨¡å¼
    if args.test:
        pending_urls = pending_urls[:3]
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†å‰ {len(pending_urls)} ä¸ªæœªå¤„ç†çš„URL")

    # é™åˆ¶æ•°é‡
    if args.limit is not None and args.limit > 0:
        pending_urls = pending_urls[:args.limit]
        print(f"ğŸ”¬ æœ¬æ¬¡ä»…å¤„ç†å‰ {len(pending_urls)} ä¸ªURL")

    print(f"ğŸ“‹ æ€»URLæ•°: {len(all_urls)}")
    print(f"âœ… å·²å¤„ç†: {len(processed_urls)}")
    print(f"â³ å¾…å¤„ç†: {len(pending_urls)}")
    print(f"ğŸ”§ å¤šçº¿ç¨‹æ¨¡å¼: {THREAD_COUNT} çº¿ç¨‹")

    if len(pending_urls) == 0:
        print("ğŸ‰ æ²¡æœ‰å¾…å¤„ç†çš„URLï¼")
        return

    # å¤šçº¿ç¨‹å¤„ç†
    start_time = time.time()
    
    try:
        with ThreadPoolExecutor(max_workers=THREAD_COUNT) as executor:
            futures = [executor.submit(process_single_attraction, url) for url in pending_urls]
            
            for i, future in enumerate(as_completed(futures)):
                try:
                    future.result()
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
                
                # æ¯å¤„ç†5ä¸ªä¿å­˜ä¸€æ¬¡è¿›åº¦
                if (i + 1) % 5 == 0:
                    save_progress()
                    print(f"ğŸ“ˆ æ‰¹é‡è¿›åº¦: {i+1}/{len(pending_urls)} ({(i+1)/len(pending_urls)*100:.1f}%)")
                    log_memory_usage()

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        save_progress()
        print("ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»ä¸­æ–­å¤„ç»§ç»­")
        return

    # æœ€ç»ˆä¿å­˜è¿›åº¦
    save_progress()

    # ç»Ÿè®¡ç»“æœ
    duration = int(time.time() - start_time)
    total_processed = len(processed_urls)
    
    print(f"\nğŸ‰ æœ¬è½®ä»»åŠ¡å®Œæˆï¼")
    print(f"ğŸ“Š æœ¬è½®å¤„ç†: {len(pending_urls)} ä¸ªæ™¯ç‚¹")
    print(f"ğŸ“Š æ€»å¤„ç†æ•°: {total_processed}/{len(all_urls)} ({(total_processed/len(all_urls)*100):.1f}%)")
    print(f"âœ… ç´¯è®¡æˆåŠŸ: {success_count}")
    print(f"âŒ ç´¯è®¡å¤±è´¥: {failed_count}")
    print(f"â±ï¸  æœ¬è½®è€—æ—¶: {duration//60:02d}:{duration%60:02d}")
    print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {OUTPUT_DIR}/")
    
    # å¦‚æœè¿˜æœ‰æœªå®Œæˆçš„ï¼Œæç¤ºç”¨æˆ·
    if total_processed < len(all_urls):
        remaining = len(all_urls) - total_processed
        print(f"\nğŸ’¡ è¿˜æœ‰ {remaining} ä¸ªURLå¾…å¤„ç†ï¼Œå¯å†æ¬¡è¿è¡Œç¨‹åºç»§ç»­")
    else:
        print(f"\nğŸ† æ‰€æœ‰ {len(all_urls)} ä¸ªæ™¯ç‚¹éƒ½å·²å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·åœæ­¢ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 